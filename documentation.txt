Introduction:
There are many great people on the internet explaining how computers work, and how to build a rudementary 
4 or 8 bit computer from a set of physical chips and wires.  As part of my college course work, I followed the same 
process to learn how computers (at least used to) work at a bit level and built my own a breadboard computer.  That process
stuck with me, and from that came the idea to try to go through the same process of designing and building a 
simple breadboard computer, but to do it completely in software instead.  This obviously has been done before as well, including remarkably for example
within the Minecraft video game:
But I have found it to be an interesting and rewarding project and a good vector to think about programming topics.  
This project was written between feb 2 and feb 11 of 2022.  While leaning on many online resources, the computer, microcode design etc are all
entirely made up by me.  All code was written by me from scratch in Ruby.

How do computers work:
A computer can be simplified as a memory bank, that is a set of 1s and 0s, and a set of devices that can be used
to implement logic gates.  Imagine a tiny device where you can hook two wires to that either have a signal on them (1, or true)
or don't (0, or false).  And one output, that produces a signal if and only if both inputs have a signal on them.
This physical machine effectively implements an "and" gate.  We can do something similar with "or" gates, "exclusive or", 
and "not" gates.
Once we have some of these gates, we can begin combining them, that is chaining them together.  With only a small set 
of types of gates..we can combine them to make any logic table we need.  For example, a 4-way and gate, a "4-way multiplexor"
which takes 4 data lines and 2 selection lines, and a single output the forwards the signal from the selected line.
The art of combining this gates together indeed is called combinatorial logic, for that reason.

This isn't quite enough.  Imagine we feed the output of a gate back into itself as an input.  Is this allowed?  What will happen if 
the logic dictates that the value change, for example a not gate fed into itself.  In reality the chip might not function, but logically
the value will never settle out.  Anything we can build from a giant set of combinatorial logic will settle out (hopefully) 
after some microseconds and give us a fixed state.  We need something more.  We also need chips that can take their own output as input,
however they ignore the input must of the time except when an 'enable' signal is triggered.  Specifically,
not when the enable signal is on, or off, but actually the moment when the signal changes from off to on (rising) or on to off (falling).
This tells when to 'lock' the value, and that value won't change until the next time the 'enable' is triggered.  Eureka, this type of 
device is called a 'data latch', and can basically form a 1-bit place to save information.

When will generate the 'enable' triggers?  A "clock".  The last physical thing we need (at least for now) is a device that can generate a square wave.
Low voltage, wait, high voltage, wait, repeat.  If we hook every data latch in our circuit up to this 'clock', they will all store their inputs in unison.
That means we can group 8 data latches together and label it as a single 8-bit data register.  Now we're computing!

There's actually another 'magic' component we could take advatage of, called a tri-state buffer.  A buffer is a simple component that does "nothing", it just has one input and one output and just passes the value along... it's like a NOT gate that doesn't flip the value.  A tri-state buffer has an extra input, and when that input is set, the regular input is ignore, and the output is...physically disconnected from the circuit.  I chose not to implement this just to avoid
this 'disconnected' state, as well as reducing the absolute number of 'physical' or 'magical' components I needed as my foundation.

Imagine we have an 8-bit data register, which has a flag to decide whether or not to 'store' the incoming information when the next clock cycle hits.  
We have other similar data registers as well.  The outputs (current values) of all of these registers flow down towards a common area, and combinatorial
logic is used to convert some other flags to decide which _one_ component is actually 'presented' as the value.  This logic is called the "bus".

We now have flags that tell different components to store what's on the bus, and more flags to decide which component is pushing a value to the bus.  

What if we create a circuit that is a register, and a set of 'Adders' that can add 1 to the register each clock cycle.  We use our flags to have that register
push to the bus, and the MAR - memory address register, read it.  The MAR is an input to a large bank of 8-bit registers that also have a set of 'address' inputs to control which of the many registers is currently being accessed.  This is RAM, random access memory.  With the MAR set, we change flags to have the RAM push to the bus, and a new IR - instruction register read in.  The IR has a set of logic that reads from it and implements 'microcode' instructions.  
A single line of machine code actually takes multiple "micro" steps to perform, which the microcode area handles.  It has its own counter, cycling through a small set of values to keep track of which micro-code we're currently running.  The output of the microcode is simply...a set of flags.  The exact
same flags we've been discussing... flags to tell component to read from the bus, and flags to tell which component is 'writing' to the bus.  Every instruction
starts with those same two microinstructions... Read from the PC and write to the MAR, then read from RAM and write to the IR, as well as telling the PC to increment.  That's it!  That is a very quick explanation of how computers work.  They are a circuit in a loop, except that data latches all hooked to a common clock keeps the computer marching forward in a nice clean series of steps instead of a single blur.


Approach:
The basic idea is to be able to implement a class that for example represents a 4-way "and" gate.  I can obviously write a function that takes 4 inputs,
does "return a & b & c & d".  However, we are looking for a  different approach where we are "simulating" how it might work using smaller subcomponents.
For example, declare an AND gate and "hook up" a and b inputs to it, do the same for c and d inputs. and then hook the outputs of those two gates together
as inputs to a third AND gate, and flag the output of this subcomponent as the overall output of this component.  In this way, we have simulated a 4-way
AND gate that should behave as we expect it to, without ever specifying its behavior in our code.  The only time we do that is for specific 'physical' components,
for example the simple 2-way ands, ors, etc.

This brings us interesting design questions.  What does it mean to "hook up" two components?  Do we simulate a bunch of wires?  Is it fair to make
wires directionaly when in reality they aren't?  Should we simulate a components power connection - for example a NOT gate generates a signal from nothing, so can't just pass on an input signal.
Does a component need to store its own intermediate input and output values, or can it pull them on the fly?  If you hook a NOT gate to itself, what happens?  What should happen?  Will the program just loop indefinitely?
If a component is just a grouping of subcomponents, in what sense is the component's inputs and outputs related to the corresponding inputs and outputs of 
the subcomponents.  As we build up more and more components, can we make something that actually resembles a simple breadboard computer.  If so, can we load programs into it and watch them run?  Will the software simulation run quickly enough to get through it?

The most important part of the logic is the update code.  We use a 'magic' update function to update all 'physical' components.  These are the only components
where we use our computer to fulfill the correct logic/behavior.  However, we need to do this in a particular way to avoid issues and handle edge cases:

Update all 'physical' components, taking note if any actually changed, meaning have a different output now than they did before.
If any components changed, repeat.  To avoid an endless loop, set an arbitrary maximum number of loops.

Tick the clock.  We actually have data latch components that operate on the rising edge of the clock, and some on the falling edge of the clock, related to the microcode components.  So have all data latches set to 'inverse' cache their inputs.  Then go through and have them all set their output to the cached value.  This is a two phase commit process to avoid having some data latch inputs starting to change before they all lock in.

Now update all component again, then tick the click high again which will take care of all normal data latches, and then lastly update one more time.  Again,
ticking the clock has some basis in reality (though in our simulation we aren't really 'wiring up' the clock), but these updates are just the behind the scenes
magic that makes physical components 'just work'.

We then just define a "computer" as a set of components, make all the connections, "load" a microcode file into the rom, and another "program" into the main RAM.  Then the PC will start at 0 and begin executing code.  At this time there's no direct input and output, so we're just watching the state of the comptuer do it's thing.

To do:
	performance
	make the clock a little more honest
	cleanup


